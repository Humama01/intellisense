# -*- coding: utf-8 -*-
"""Copy of FYPur2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ABIUhPOBHXhtB968cmJYVyc-_On2hN_Q

> **Translation Transcription Libraries Install**
"""

!pip install gradio
!pip install git+https://www.github.com/openai/whisper.git
!apt-get install ffmpeg

"""






> **Translation Transcription to File Generation**

"""

#interface beautify, analysis function, analysis output in PDf file stored

import time
import asyncio
import gradio as gr
import subprocess
import os

ofile="/content/Test_Audio.srt"
def transcribe_or_translate(input_audio):
    try:
        # Get the absolute path of the audio file and derive the output file path
        audio_path = os.path.abspath(input_audio)
        output_file = os.path.join("/content", os.path.splitext(os.path.basename(input_audio))[0] + ".srt")
        ofile=output_file
        # Remove any existing output file to prevent conflicts
        if os.path.exists(output_file):
            os.remove(output_file)

        # Run Whisper CLI to transcribe/translate to Arabic
        command = f'whisper "{audio_path}" --language en --task transcribe --model large-v2'
        result = subprocess.run(command, shell=True, capture_output=True, text=True)

        # Check if the command ran successfully
        if result.returncode != 0:
            return f"Error: {result.stderr}"

        # Read and return the output from the generated file
        with open(output_file, "r", encoding="utf-8") as file:
            translated_text = file.read()

        return translated_text

    except Exception as e:
        # Catch and return any other exceptions
        return f"Error: {str(e)}"





# Define the generate_anlysis function
def generate_analysis(transcribed_text):
  {}
    # Here you can implement your logic for analysis or summarization.
    # For example, a simple sentiment analysis placeholder:
#    if "good" in transcribed_text.lower():
#        return "The sentiment is positive."
#    elif "bad" in transcribed_text.lower():
#        return "The sentiment is negative."
#    else:
#        return "The sentiment is neutral."





 #Create a Gradio Interface for the app
interface = gr.Interface(
    fn=transcribe_or_translate,
    inputs=gr.Audio(type="filepath"),
    outputs="text",
    title="AI-based Transcription, Translation, Sentiment Analysis & Visualizations",
    description="Pipeline for Monitoring and Analysing Social and Electronic Media"
)








# Launch the app
interface.launch()

ofile="/content/Test_Audio.srt"
# Define the file path of the generated transcript file
file_path = ofile #os.path.join("/content", os.path.splitext(os.path.basename(input_audio))[0] + ".srt")  # Adjust the path if needed (e.g., .srt, .tsv)


# Wait until the transcript file is created
while not os.path.exists(file_path):
    #print("Waiting for transcription to complete...")
    time.sleep(20)  # Check every 120 seconds if the file exists

# Read the content of the transcript file
with open(file_path, "r", encoding="utf-8") as file:
    file_content = file.read()
    time.sleep(1)






async def main2():
    # Custom print function to display output in a colored box
    def print_colored_box(text):
        print("\033[96m" + "=" * 80)   # Cyan color for the top border
        print(" " * 10 + "\033[1;97m" + "ANALYSIS REPORT" + " " * 10)   # Bold white title centered # Intelligent Report for Security Analysis
        print("\033[96m" + "=" * 80)   # Cyan color for the bottom border
        print("\033[0;37m" + text + "\033[0m")   # Light gray for the text content
        print("\033[96m" + "=" * 80 + "\033[0m")   # Cyan color for bottom border reset to default

    # Await the asynchronous generate_answer function
    genn = generate_analysis(f"""{file_content}""")   #   , """ {scr} """

    # Wait for a short period to simulate any delay (using async sleep)
    await asyncio.sleep(10)

    # Print the result in a colored box
    print_colored_box(genn)

# Run the asynchronous main2 function
await main2()

"""


> **Libraries Install for Text Analysis**


"""

# Install necessary libraries
!pip install pysrt pandas transformers nltk spacy feel-it textblob vaderSentiment pysentimiento matplotlib nltk wordcloud streamlit bertopic keybert
!python -m spacy download en_core_web_sm

"""




> **Evaluative Analysis & Visualizations**


"""

#latest
import pysrt
import pandas as pd
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from pysentimiento import create_analyzer
import spacy


from wordcloud import WordCloud
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords




# Load the SRT file and extract text
def load_srt(file_path):
    subs = pysrt.open(file_path)
    text_data = [{'start_time': sub.start.to_time(), 'end_time': sub.end.to_time(), 'text': sub.text} for sub in subs]
    return pd.DataFrame(text_data)

# Load your SRT file
ofile2="/content/Mossad.srt"

srt_df = load_srt(ofile2)

# Initialize RoBERTa Sentiment Analysis
roberta_model_name = "cardiffnlp/twitter-roberta-base-sentiment" #"bhadresh-savani/robertuito-sentiment-analysis"  #pysentimiento/roberta-targeted-sentiment-analysis
roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)
roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name)
roberta_analyzer = pipeline(
    "text-classification",
    model=roberta_model,
    tokenizer=roberta_tokenizer,
    truncation=True,  # Ensure truncation happens at pipeline level
    max_length=512,   # Set the maximum token length
    return_all_scores=True
)

# Define a mapping for RoBERTa sentiment labels
roberta_label_map = {
    "LABEL_0": "Negative",
    "LABEL_1": "Neutral",
    "LABEL_2": "Positive"
}

# Initialize Pysentimiento Sentiment Analysis
pysentimiento_analyzer = create_analyzer(task="sentiment", lang="en")

# Initialize Emotion Detection (bhadresh-savani/distilbert-base-uncased-emotion)
emotion_model_name = "bhadresh-savani/distilbert-base-uncased-emotion"
emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_name)
emotion_analyzer = pipeline(
    "text-classification",
    model=emotion_model_name,
    tokenizer=emotion_tokenizer,
    truncation=True,  # Ensure truncation happens at pipeline level
    max_length=512,   # Set the maximum token length
    return_all_scores=True
)

# Initialize Feel-It Sentiment Analysis
feelit_model_name = "MilaNLProc/feel-it-italian-sentiment"
feelit_tokenizer = AutoTokenizer.from_pretrained(feelit_model_name)
feelit_model = AutoModelForSequenceClassification.from_pretrained(feelit_model_name)
feelit_analyzer = pipeline(
    "text-classification",
    model=feelit_model,
    tokenizer=feelit_tokenizer,
    truncation=True,  # Ensure truncation happens at pipeline level
    max_length=512,   # Set the maximum token length
    return_all_scores=True
)

# Initialize SpaCy for NER
nlp = spacy.load("en_core_web_sm")

# Helper function to clean text
def clean_text(text):
    """
    Cleans input text by removing special characters and emojis.
    """
    import re
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove excessive whitespace
    return text

# Prepare full text and clean it
full_text = " ".join(srt_df['text'])
cleaned_text = clean_text(full_text)

# Perform RoBERTa sentiment analysis
roberta_results = roberta_analyzer(cleaned_text)
roberta_prediction = max(roberta_results[0], key=lambda x: x['score'])
roberta_sentiment = roberta_label_map[roberta_prediction['label']]  # Map LABEL_X to meaningful labels

# Perform Pysentimiento sentiment analysis
pysentimiento_results = pysentimiento_analyzer.predict(cleaned_text)

# Perform emotion detection
emotion_results = emotion_analyzer(cleaned_text)
dominant_emotion = max(emotion_results[0], key=lambda x: x['score'])

# Perform Feel-It sentiment analysis
feelit_results = feelit_analyzer(cleaned_text)
feelit_prediction = max(feelit_results[0], key=lambda x: x['score'])

# Perform NER with SpaCy
entities = [(ent.text, ent.label_) for ent in nlp(cleaned_text).ents]

# Compile results into a summary table
results = [
    {"Metric": "RoBERTa Sentiment", "Details": f"{roberta_sentiment} (confidence: {roberta_prediction['score']:.2f})"},
    {"Metric": "Pysentimiento Sentiment", "Details": pysentimiento_results.output},
    {"Metric": "Dominant Emotion", "Details": dominant_emotion['label']},
    {"Metric": "Feel-It Sentiment", "Details": feelit_prediction['label']},
    {"Metric": "Named Entities", "Details": ", ".join([f"{ent[0]} ({ent[1]})" for ent in entities])}
]

# Convert results to a DataFrame for tabular display
results_df = pd.DataFrame(results)

# Print the results
print(results_df)

roberta_neg_score = roberta_results[0][0]['score']  # Assuming LABEL_0 corresponds to "Negative"
roberta_sentiment = roberta_label_map[roberta_results[0][0]['label']]  # Map to sentiment label

# if the sentiment is above 0.7 NEG, mark it as red or generate a msg that negative view present (later can be includud that if stuff is Pro Indian let's suppose, threat flag generate,..)
# Check if negative sentiment score exceeds 0.7 and sentiment is Negative
if roberta_neg_score > 0.7 and roberta_sentiment == 'Negative':
    print("\n\n⚠️ Flag: The content is highly negative. Confidence: {:.2f}".format(roberta_neg_score))
else:
    print("\n\n✅ The content is not significantly negative.")



# Save the results to a CSV file
results_df.to_csv("/content/analysis_results.csv", index=False)

"""
> **Ploting Visualizations**

"""

import matplotlib.pyplot as plt

# Count entity types
entity_counts = pd.Series([ent[1] for ent in entities]).value_counts()

# Plot bubble chart
plt.figure(figsize=(8, 6))
plt.scatter(entity_counts.index, entity_counts.values, s=entity_counts.values * 100, alpha=0.6, color="skyblue")
plt.title("Bubble Chart of Entity Types")
plt.xlabel("Entity Types")
plt.ylabel("Frequency")
plt.show()

plt.figure(figsize=(8, 1))  # Blank figure for spacing




# Download stopwords
nltk.download('stopwords')

# Create WordCloud
wordcloud = WordCloud(width=800, height=400,
                      background_color='white',
                      stopwords=set(stopwords.words('english')),colormap='viridis',
                      min_font_size=10).generate(cleaned_text)

# Display the generated image
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Word Cloud of File", fontsize=16)
plt.tight_layout(pad=0)

# Save the word cloud image
plt.savefig('/content/wordcloud.png', dpi=300, bbox_inches='tight')
plt.show()







import numpy as np

# Split text into chunks and analyze sentiment
chunk_size = len(cleaned_text) // 10
chunks = [cleaned_text[i:i + chunk_size] for i in range(0, len(cleaned_text), chunk_size)]
chunk_sentiments = [roberta_analyzer(chunk) for chunk in chunks]
chunk_scores = [max(result[0], key=lambda x: x['score'])['score'] for result in chunk_sentiments]

# Plot sentiment scores over time
plt.figure(figsize=(10, 6))
plt.plot(range(len(chunk_scores)), chunk_scores, marker="o", linestyle="--", color="green")
plt.title("Sentiment Over Time")
plt.xlabel("Chunk Index")
plt.ylabel("Sentiment Score")
plt.show()

plt.figure(figsize=(8, 1))  # Blank figure for spacing




from collections import Counter

# Extract named entities dynamically
entity_texts = [ent[0] for ent in entities]

# Count occurrences of entities
entity_counts = Counter(entity_texts)

# Generate word cloud
entity_wordcloud = WordCloud(
    width=800, height=400, background_color='white', colormap='copper'
).generate_from_frequencies(entity_counts)

plt.figure(figsize=(10, 5))
plt.imshow(entity_wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Named Entity Word Cloud", fontsize=16)
plt.tight_layout(pad=0)
plt.show()

plt.figure(figsize=(8, 1))  # Blank figure for spacing






import seaborn as sns

# Prepare entity frequency data
entity_df = pd.DataFrame(entity_counts.items(), columns=["Entity", "Count"]).sort_values(by="Count", ascending=False)

# Plot bar chart
plt.figure(figsize=(10, 6))
sns.barplot(data=entity_df, x="Count", y="Entity", palette="Blues_r")
plt.title("Named Entity Frequency")
plt.xlabel("Count")
plt.ylabel("Entity")
plt.show()

plt.figure(figsize=(8, 1))  # Blank figure for spacing






# Extract sentiment and emotion labels dynamically
sentiment_labels = [roberta_label_map[result['label']] for result in roberta_results[0]]
emotion_labels = [result['label'] for result in emotion_results[0]]

# Create a heatmap matrix dynamically
heatmap_matrix = np.outer(
    [result['score'] for result in roberta_results[0]],  # Sentiment scores
    [result['score'] for result in emotion_results[0]]   # Emotion scores
)

# Generate the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(
    heatmap_matrix, annot=True, cmap="coolwarm",
    xticklabels=emotion_labels, yticklabels=sentiment_labels
)
plt.title("Sentiment-Emotion Heatmap")
plt.xlabel("Emotions")
plt.ylabel("Sentiments")
plt.show()

plt.figure(figsize=(8, 1))  # Blank figure for spacing









import plotly.graph_objects as go

# Prepare links dynamically
links = []
for i, entity in enumerate(entity_texts):
    links.append({"source": i, "target": len(entity_texts), "value": 1})  # Example association

# Create Sankey diagram
sankey_data = go.Sankey(
    node=dict(label=entity_texts + [roberta_sentiment]),
    link=dict(
        source=[link['source'] for link in links],
        target=[link['target'] for link in links],
        value=[link['value'] for link in links]
    )
)

fig = go.Figure(sankey_data)
fig.update_layout(title_text="NER-Sentiment Sankey Diagram", font_size=10)
fig.show()

plt.figure(figsize=(8, 1))  # Blank figure for spacing





# Define chunks of text
chunk_size = len(cleaned_text) // 10
chunks = [cleaned_text[i:i + chunk_size] for i in range(0, len(cleaned_text), chunk_size)]

# Track emotions dynamically over chunks
emotion_data = {emotion: [] for emotion in emotion_labels}
time_intervals = range(1, len(chunks) + 1)

for chunk in chunks:
    chunk_results = emotion_analyzer(chunk)
    for result in chunk_results[0]:
        emotion_data[result['label']].append(result['score'])

# Convert to DataFrame
df = pd.DataFrame(emotion_data, index=time_intervals)

# Plot stacked area chart
df.plot.area(alpha=0.5, figsize=(10, 6), colormap="Set2")
plt.title("Emotion Over Time")
plt.xlabel("Time Interval")
plt.ylabel("Emotion Intensity")
plt.show()

import matplotlib.pyplot as plt
from math import pi
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import scipy.cluster.hierarchy as sch
import streamlit as st

# Step 1: Initialize the sentiment score accumulator
sentiment_scores = {"Positive": 0, "Neutral": 0, "Negative": 0}

# Step 2: Accumulate scores from RoBERTa results
for result in roberta_results[0]:
    label = roberta_label_map[result['label']]  # Convert label to human-readable form
    sentiment_scores[label] += result['score']

# Step 3: Prepare data for visualization
labels = sentiment_scores.keys()
sizes = sentiment_scores.values()
colors = ['lightgreen', 'gold', 'salmon']

# Step 4: Plot the pie chart and save it to a figure object
fig_sentiment_pie, ax = plt.subplots(figsize=(8, 6))
ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)
ax.set_title("Sentiment Distribution (RoBERTa)")
plt.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle.

# Step 5: Display the pie chart in Streamlit
st.subheader("Sentiment Distribution")
st.pyplot(fig_sentiment_pie)











# Step 1: Aggregate emotion scores across chunks
# Define chunks of text
chunk_size = len(cleaned_text) // 10
chunks = [cleaned_text[i:i + chunk_size] for i in range(0, len(cleaned_text), chunk_size)]

# Initialize emotion data structure
emotion_data = {emotion: [] for emotion in emotion_labels}

# Collect emotion scores for each chunk
for chunk in chunks:
    chunk_results = emotion_analyzer(chunk)
    for result in chunk_results[0]:
        emotion_data[result['label']].append(result['score'])

# Step 2: Compute average emotion scores across all chunks
average_emotion_scores = {emotion: sum(scores) / len(scores) if scores else 0 for emotion, scores in emotion_data.items()}

# Step 3: Update the radar chart function to use average scores
def emotion_radar_chart(emotion_labels, emotion_scores):
    num_vars = len(emotion_labels)

    # Prepare data
    angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]
    emotion_scores_scaled = MinMaxScaler().fit_transform(np.array(emotion_scores).reshape(-1, 1)).flatten()

    # Complete the loop to close the radar chart
    emotion_scores_scaled = np.concatenate([emotion_scores_scaled, [emotion_scores_scaled[0]]])
    angles += angles[:1]

    fig_emotion_radar, ax = plt.subplots(figsize=(8, 8), dpi=80, subplot_kw=dict(polar=True))
    ax.set_theta_offset(pi / 2)
    ax.set_theta_direction(-1)

    # Plot and fill the radar chart
    ax.plot(angles, emotion_scores_scaled, linewidth=2, linestyle='solid')
    ax.fill(angles, emotion_scores_scaled, color='skyblue', alpha=0.4)

    # Set labels and title
    plt.xticks(angles[:-1], emotion_labels, color='black', size=12)
    plt.title("Emotion Radar Chart (Spider Chart)", size=16)
    plt.show()

    return fig_emotion_radar

# Step 4: Generate the radar chart using the average scores
fig_emotion_radar = emotion_radar_chart(list(average_emotion_scores.keys()), list(average_emotion_scores.values()))

# Step 5: Display the radar chart in Streamlit
st.subheader("Emotion Radar Chart (Average Intensity Across Chunks)")
st.pyplot(fig_emotion_radar)













# Aggregate emotion scores across chunks (same as radar chart fix)
chunk_size = len(cleaned_text) // 10
chunks = [cleaned_text[i:i + chunk_size] for i in range(0, len(cleaned_text), chunk_size)]

# Initialize emotion data structure
emotion_data = {emotion: [] for emotion in emotion_labels}

# Collect emotion scores for each chunk
for chunk in chunks:
    chunk_results = emotion_analyzer(chunk)
    for result in chunk_results[0]:
        emotion_data[result['label']].append(result['score'])

# Compute average emotion scores across all chunks
average_emotion_scores = {emotion: sum(scores) / len(scores) if scores else 0 for emotion, scores in emotion_data.items()}

# Convert to labels and scores for dendrogram
emotion_labels = list(average_emotion_scores.keys())
emotion_scores = list(average_emotion_scores.values())

# Function to create a Hierarchical Dendrogram for Emotions
def hierarchical_dendrogram(emotion_labels, emotion_scores):
    # Reshape scores to 2D array for linkage matrix
    emotion_scores_reshaped = np.array(emotion_scores).reshape(-1, 1)

    # Generate the linkage matrix
    linkage_matrix = sch.linkage(emotion_scores_reshaped, method='ward')

    plt.figure(figsize=(10, 6))
    sch.dendrogram(
        linkage_matrix,
        labels=emotion_labels,
        color_threshold=0.5
    )
    plt.title("Hierarchical Dendrogram for Emotions")
    plt.xlabel("Emotions")
    plt.ylabel("Distance")
    plt.show()

# Call the dendrogram function with corrected inputs
hierarchical_dendrogram(emotion_labels, emotion_scores)

"""

> **Summary Generation**

"""

from transformers import pipeline


def split_text(text, max_length=1024):
    """
    Split text into chunks of a given maximum length.
    """
    import re
    sentences = re.split(r'(?<=[.!?]) +', text)  # Split into sentences
    chunks, current_chunk = [], []

    for sentence in sentences:
        if len(" ".join(current_chunk + [sentence])) <= max_length:
            current_chunk.append(sentence)
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

def generate_title_and_summary(text, max_summary_length=50):
    # Split text into manageable chunks
    text_chunks = split_text(text)

    # Summarize each chunk
    chunk_summaries = [
        summarizer(chunk, max_length=max_summary_length, min_length=20, do_sample=False)[0]['summary_text']
        for chunk in text_chunks
    ]

    # Combine chunk summaries into a single summary
    combined_summary = " ".join(chunk_summaries)

    # Generate a title from the combined summary
    title = summarizer(combined_summary, max_length=10, min_length=5, do_sample=False)[0]['summary_text']

    return title, combined_summary

# Generate the title and summary
short_text = cleaned_text[:1024]  # Truncate the text to fit the model's input limit

title, summary = generate_title_and_summary(cleaned_text)

print(f"Title: {title}")
print(f"Summary: {summary}")

"""

> **Topic Modeling**

"""

from bertopic import BERTopic
from nltk.tokenize import sent_tokenize  # For splitting text into sentences
import nltk  # Import nltk for downloading the tokenizer
import numpy as np

# Download the required NLTK tokenizer
nltk.download('punkt', force=True)

# Split the cleaned text into chunks (sub-documents) for better topic extraction
def split_into_chunks(text, max_length=500):
    """
    Splits text into chunks of approximately `max_length` tokens.
    """
    sentences = sent_tokenize(text)  # Split text into sentences
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk.split()) + len(sentence.split()) > max_length:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += " " + sentence

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

# Split cleaned_text into chunks
chunks = split_into_chunks(cleaned_text)

# Initialize and fit the BERTopic model
topic_model = BERTopic(min_topic_size=5)  # Set minimum topic size to filter out noise
topics, probs = topic_model.fit_transform(chunks)

# Get the top topics and their keywords
top_topics = topic_model.get_topic_info()

# Print top topics if available
if len(top_topics) > 0:
    print("Top Topics:")
    print(top_topics)

    # Visualize the top 5 topics in a bar chart
    topic_model.visualize_barchart(top_n_topics=min(5, len(top_topics))).show()
else:
    print("No valid topics were found. Please ensure the input text is sufficient and coherent.")

import pysrt
import pandas as pd
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from pysentimiento import create_analyzer
import spacy
from bertopic import BERTopic
import re
import textwrap
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Load the SRT file and extract text
def load_srt(file_path):
    subs = pysrt.open(file_path)
    text_data = [{'start_time': sub.start.to_time(), 'end_time': sub.end.to_time(), 'text': sub.text} for sub in subs]
    return pd.DataFrame(text_data)

# Load your SRT file
ofile2 = "/content/Mossad.srt"
srt_df = load_srt(ofile2)

# Helper function to clean text
def clean_text(text):
    """
    Cleans input text by removing special characters and emojis.
    """
    import re
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove excessive whitespace
    return text

# Prepare full text and clean it
full_text = " ".join(srt_df['text'])
cleaned_text = clean_text(full_text)

print(f"Total length of cleaned_text: {len(cleaned_text)}")
print(f"Cleaned Text Preview: {cleaned_text[:500]}")

### ---------------- CHUNKING METHODS ---------------- ###

# Method 1: Splitting text using regular expressions (sentences-based)
def split_into_chunks_regex(text, max_length=500):
    sentences = re.split(r'(?<=[.!?]) +', text)
    chunks = []
    current_chunk = ''
    for sentence in sentences:
        if len(current_chunk.split()) + len(sentence.split()) > max_length:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += ' ' + sentence
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

# Method 2: Splitting text into fixed-length chunks (by word count)
def split_into_chunks_by_length(text, chunk_size=500):
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# Method 3: Using textwrap to split text into chunks of fixed length (character count)
def split_into_chunks_with_textwrap(text, chunk_size=500):
    return textwrap.wrap(text, chunk_size)

### Select Chunking Method (Uncomment One Method Below) ###
chunks = split_into_chunks_regex(cleaned_text, max_length=500)
# chunks = split_into_chunks_by_length(cleaned_text, chunk_size=500)
# chunks = split_into_chunks_with_textwrap(cleaned_text, chunk_size=500)

print(f"Number of Chunks: {len(chunks)}")
print(f"Sample Chunk: {chunks[:1]}")  # Print the first chunk for inspection

# Check if chunks are empty
if len(chunks) == 0:
    raise ValueError("Chunks list is empty! Ensure the input text is sufficient.")

### ---------------- BERTopic Topic Modeling ---------------- ###
from bertopic import BERTopic
from umap import UMAP

# Custom UMAP configuration for small datasets
custom_umap = UMAP(n_neighbors=2, n_components=2, min_dist=0.1, metric='cosine')

# Initialize BERTopic
topic_model = BERTopic(umap_model=custom_umap, min_topic_size=2, calculate_probabilities=True)

# Initialize and fit the BERTopic model
#topic_model = BERTopic(min_topic_size=5)  # Set minimum topic size to filter out noise
#topics, probs = topic_model.fit_transform(chunks)


# Fit the model on the chunks
topics, probs = topic_model.fit_transform(chunks)

# Get top topics
top_topics = topic_model.get_topic_info()
print("Top Topics:", top_topics)

# Get the top topics and their keyword
#top_topics = topic_model.get_topic_info()




# Print top topics if available
if len(top_topics) > 0:
    print("Top Topics:")
    print(top_topics)

    # Visualize the top 5 topics in a bar chart
    topic_model.visualize_barchart(top_n_topics=min(5, len(top_topics))).show()
else:
    print("No valid topics were found. Please ensure the input text is sufficient and coherent.")

import pysrt
import pandas as pd
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from pysentimiento import create_analyzer
import spacy
from bertopic import BERTopic
from umap import UMAP
import re
import textwrap
import numpy as np
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# -------------------- Load SRT File and Extract Text -------------------- #
def load_srt(file_path):
    subs = pysrt.open(file_path)
    text_data = [{'start_time': sub.start.to_time(), 'end_time': sub.end.to_time(), 'text': sub.text} for sub in subs]
    return pd.DataFrame(text_data)

# Load your SRT file
ofile2 = "/content/Mossad.srt"
srt_df = load_srt(ofile2)

# -------------------- Helper Function to Clean Text -------------------- #
def clean_text(text):
    """
    Cleans input text by removing special characters and emojis.
    """
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove excessive whitespace
    return text

# Prepare full text and clean it
full_text = " ".join(srt_df['text'])
cleaned_text = clean_text(full_text)

print(f"Total length of cleaned_text: {len(cleaned_text)}")
print(f"Cleaned Text Preview: {cleaned_text[:500]}")

# -------------------- Chunking Methods -------------------- #
# Method 1: Splitting text using regular expressions (sentences-based)
def split_into_chunks_regex(text, max_length=500):
    sentences = re.split(r'(?<=[.!?]) +', text)
    chunks = []
    current_chunk = ''
    for sentence in sentences:
        if len(current_chunk.split()) + len(sentence.split()) > max_length:
            chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += ' ' + sentence
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

# Method 2: Splitting text into fixed-length chunks (by word count)
def split_into_chunks_by_length(text, chunk_size=500):
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# Method 3: Using textwrap to split text into chunks of fixed length (character count)
def split_into_chunks_with_textwrap(text, chunk_size=500):
    return textwrap.wrap(text, chunk_size)

# -------------------- Select Chunking Method -------------------- #
chunks = split_into_chunks_regex(cleaned_text, max_length=500)
# chunks = split_into_chunks_by_length(cleaned_text, chunk_size=500)
# chunks = split_into_chunks_with_textwrap(cleaned_text, chunk_size=500)

print(f"Number of Chunks: {len(chunks)}")
print(f"Sample Chunk: {chunks[:1]}")  # Print the first chunk for inspection

# -------------------- Combine Small Chunks -------------------- #
def combine_small_chunks(chunks, min_chunk_size=100):
    """
    Combines small chunks into larger ones if their length is below the threshold.
    """
    combined_chunks = []
    temp_chunk = ""

    for chunk in chunks:
        if len(chunk.split()) < min_chunk_size:
            temp_chunk += " " + chunk
        else:
            if temp_chunk:
                combined_chunks.append(temp_chunk.strip())
                temp_chunk = ""
            combined_chunks.append(chunk)

    # Add the last combined chunk if any
    if temp_chunk:
        combined_chunks.append(temp_chunk.strip())

    return combined_chunks

# Combine small chunks and reprocess
chunks = combine_small_chunks(chunks, min_chunk_size=100)
print(f"Number of Combined Chunks: {len(chunks)}")

# -------------------- Check for Empty Chunks -------------------- #
if len(chunks) == 0 or all(len(chunk.split()) < 5 for chunk in chunks):
    raise ValueError("Chunks are empty or too short! Ensure the input text has enough content.")

# -------------------- BERTopic Topic Modeling -------------------- #
# Custom UMAP configuration for small datasets
custom_umap = UMAP(
    n_neighbors=2,         # Fewer neighbors for small datasets
    n_components=2,        # Low-dimensional embedding space
    min_dist=0.1,          # Allow closer clusters
    metric='cosine'        # Cosine distance for text similarity
)

# Initialize BERTopic with custom UMAP
topic_model = BERTopic(
    umap_model=custom_umap,
    min_topic_size=2,            # Reduce minimum topic size
    calculate_probabilities=True # Return probabilities for topic distribution
)

# Fit the model on the chunks
try:
    topics, probs = topic_model.fit_transform(chunks)
except ValueError as e:
    print(f"Error during topic modeling: {e}")
    print("Ensure chunks have sufficient content and are not too short.")

# -------------------- Get and Display Top Topics -------------------- #
top_topics = topic_model.get_topic_info()

if len(top_topics) > 0:
    print("Top Topics:")
    print(top_topics)

    # Visualize the top 5 topics in a bar chart
    topic_model.visualize_barchart(top_n_topics=min(5, len(top_topics))).show()
else:
    print("No valid topics were found. Please ensure the input text is sufficient and coherent.")

import pysrt
import pandas as pd
from bertopic import BERTopic
from umap import UMAP
import re

# -------------------- Load SRT File and Extract Text -------------------- #
def load_srt(file_path):
    subs = pysrt.open(file_path)
    text_data = [{'start_time': sub.start.to_time(), 'end_time': sub.end.to_time(), 'text': sub.text} for sub in subs]
    return pd.DataFrame(text_data)

# Load your SRT file
ofile2 = "/content/Mossad.srt"
srt_df = load_srt(ofile2)

# -------------------- Helper Function to Clean Text -------------------- #
def clean_text(text):
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove excessive whitespace
    return text

# Prepare full text and clean it
full_text = " ".join(srt_df['text'])
cleaned_text = clean_text(full_text)
print(f"Total length of cleaned_text: {len(cleaned_text)}")

# -------------------- Chunking Method (Increase Chunk Size) -------------------- #
def split_into_large_chunks(text, chunk_size=1000):
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

chunks = split_into_large_chunks(cleaned_text, chunk_size=1000)
print(f"Number of Chunks: {len(chunks)}")

# Debugging: Print chunk details
for i, chunk in enumerate(chunks):
    print(f"Chunk {i+1} (Length: {len(chunk.split())} words):")
    print(chunk[:300])  # Print first 300 characters of each chunk

# -------------------- Check for Valid Chunks -------------------- #
if len(chunks) < 1 or all(len(chunk.split()) < 5 for chunk in chunks):
    raise ValueError("Chunks are too short or empty! Ensure the input text has enough content.")

# -------------------- BERTopic Topic Modeling -------------------- #
custom_umap = UMAP(n_neighbors=5, n_components=2, min_dist=0.1, metric='cosine')

# Initialize BERTopic with custom UMAP
topic_model = BERTopic(umap_model=custom_umap, min_topic_size=2, calculate_probabilities=True)

try:
    # Fit the model on the chunks
    topics, probs = topic_model.fit_transform(chunks)
    top_topics = topic_model.get_topic_info()

    if len(top_topics) > 0:
        print("Top Topics:")
        print(top_topics)

        # Visualize the top topics
        topic_model.visualize_barchart(top_n_topics=min(5, len(top_topics))).show()
    else:
        print("No valid topics were found. Please ensure the input text is sufficient and coherent.")

except ValueError as e:
    print(f"Error during topic modeling: {e}")
    print("Trying to combine all text into one chunk as a fallback...")

    # Fallback: Combine all chunks and retry topic modeling
    combined_text = " ".join(chunks)
    fallback_chunks = [combined_text]

    try:
        topics, probs = topic_model.fit_transform(fallback_chunks)
        top_topics = topic_model.get_topic_info()
        print("Top Topics (Fallback):")
        print(top_topics)

        # Visualize fallback topics
        topic_model.visualize_barchart(top_n_topics=min(5, len(top_topics))).show()
    except ValueError as fallback_e:
        print(f"Fallback topic modeling also failed: {fallback_e}")
        print("Please provide more text data or ensure the text is diverse and meaningful.")

import pysrt
import pandas as pd
from bertopic import BERTopic
from umap import UMAP
import re
import textwrap

# -------------------- Load SRT file and clean text -------------------- #
def load_srt(file_path):
    """
    Load the SRT file and return a DataFrame containing the text, start, and end times.
    """
    subs = pysrt.open(file_path)
    text_data = [{'start_time': sub.start.to_time(), 'end_time': sub.end.to_time(), 'text': sub.text} for sub in subs]
    return pd.DataFrame(text_data)

def clean_text(text):
    """
    Cleans input text by removing special characters and emojis.
    """
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove excessive whitespace
    return text

# Load SRT file
ofile2 = "/content/Mossad.srt"
srt_df = load_srt(ofile2)

# Prepare full text and clean it
full_text = " ".join(srt_df['text'])
cleaned_text = clean_text(full_text)
print(f"Total length of cleaned_text: {len(cleaned_text.split())} words")

# -------------------- Chunking Functions -------------------- #
def split_into_chunks_by_length(text, chunk_size=500):
    """
    Splits the text into chunks based on a fixed number of words.
    """
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# Split cleaned text into chunks of 500 words each
chunks = split_into_chunks_by_length(cleaned_text, chunk_size=500)
print(f"Number of Chunks: {len(chunks)}")

# Display the first two chunks for inspection
for i, chunk in enumerate(chunks[:2]):
    print(f"\nChunk {i+1} (Length: {len(chunk.split())} words):")
    print(chunk[:300] + "...\n")

# -------------------- BERTopic Topic Modeling -------------------- #
custom_umap = UMAP(n_neighbors=2, n_components=2, min_dist=0.1, metric='cosine')

# Initialize BERTopic with custom UMAP
topic_model = BERTopic(umap_model=custom_umap, min_topic_size=5, calculate_probabilities=True)

# Fit the BERTopic model on the chunks
try:
    topics, probs = topic_model.fit_transform(chunks)

    # Get top topics and display them
    top_topics = topic_model.get_topic_info()
    if len(top_topics) > 0:
        print("\nTop Topics:")
        print(top_topics)
        # Visualize the top 5 topics in a bar chart
        topic_model.visualize_barchart(top_n_topics=min(5, len(top_topics))).show()
    else:
        print("No valid topics were found.")
except Exception as e:
    print(f"Error during topic modeling: {e}")

import pandas as pd
from bertopic import BERTopic
from umap import UMAP
import re

# -------------------- Load and Clean .txt File -------------------- #
def load_txt(file_path):
    """
    Load a .txt file and return the text content.
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    return text

def clean_text(text):
    """
    Cleans input text by removing special characters and emojis.
    """
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)  # Remove non-ASCII characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove excessive whitespace
    return text

# Load and clean text from the .txt file
file_path = "/content/Moss.txt"  # Update this path to your .txt file
full_text = load_txt(file_path)
cleaned_text = clean_text(full_text)
print(f"Total length of cleaned_text: {len(cleaned_text.split())} words")

# -------------------- Chunking Functions -------------------- #
def split_into_chunks_by_length(text, chunk_size=500):
    """
    Splits the text into chunks based on a fixed number of words.
    """
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

# Split cleaned text into chunks of 500 words each
chunks = split_into_chunks_by_length(cleaned_text, chunk_size=500)
print(f"Number of Chunks: {len(chunks)}")

# Display the first two chunks for inspection
for i, chunk in enumerate(chunks[:2]):
    print(f"\nChunk {i+1} (Length: {len(chunk.split())} words):")
    print(chunk[:300] + "...\n")

# -------------------- BERTopic Topic Modeling -------------------- #
custom_umap = UMAP(n_neighbors=2, n_components=2, min_dist=0.1, metric='cosine')

# Initialize BERTopic with custom UMAP
topic_model = BERTopic(umap_model=custom_umap, min_topic_size=5, calculate_probabilities=True)

# Fit the BERTopic model on the chunks
try:
    topics, probs = topic_model.fit_transform(chunks)

    # Get top topics and display them
    top_topics = topic_model.get_topic_info()
    if len(top_topics) > 0:
        print("\nTop Topics:")
        print(top_topics)
        # Visualize the top 5 topics in a bar chart
        topic_model.visualize_barchart(top_n_topics=min(5, len(top_topics))).show()
    else:
        print("No valid topics were found.")
except Exception as e:
    print(f"Error during topic modeling: {e}")

from keybert import KeyBERT

# Initialize KeyBERT model
kw_model = KeyBERT()

# Extract keywords from the text
keywords = kw_model.extract_keywords(cleaned_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=10)

print("Top Keywords:", keywords)

"""

> **Word Flagging**

"""

#take input of words/lists through UI, then check if those words were present in the file, if yes, tell the duration, numbr of times, and the whole sentence as text.

# Function to search for specific words and provide details
def monitor_words_in_srt(df, word_list):
    results = []
    word_count = {word: 0 for word in word_list}  # Initialize word frequency count

    for _, row in df.iterrows():
        start_time = row['start_time']
        end_time = row['end_time']
        text = row['text'].lower()

        # Check if any word from the list appears in the text
        for word in word_list:
            if word.lower() in text:
                word_count[word] += text.count(word.lower())
                results.append({
                    "Word": word,
                    "Start Time": start_time,
                    "End Time": end_time,
                    "Context": text
                })

    # Generate flags for words found
    for word, count in word_count.items():
        if count > 0:
            print(f"⚠️ Flag: The word '{word}' was used {count} times.")
        else:
            print(f"✅ The word '{word}' was not found.")

    # Display flagged occurrences with context
    flagged_df = pd.DataFrame(results)
    if not flagged_df.empty:
        print("\nFlagged Word Occurrences with Context:")
        print(flagged_df)
        flagged_df.to_csv("/content/flagged_word_occurrences.csv", index=False)
    else:
        print("\nNo flagged words found.")

# User-defined list of words to monitor
word_list = ["war", "terror", "attack"]  # Modify this list as needed

# Monitor the words in the SRT file
monitor_words_in_srt(srt_df, word_list)

!pip install flask_cors

!pip install FPDF

!pip install pyngrok

!pip install nltk

import os
import io
import time
import torch
import whisper
import warnings
import base64
import subprocess
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from wordcloud import WordCloud
from fpdf import FPDF
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from transformers import pipeline
from moviepy.editor import VideoFileClip
from pyngrok import ngrok
from collections import Counter
from sklearn.preprocessing import MinMaxScaler
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning)

# Additional import for Named Entity Recognition
import spacy

# Initialize Flask App
app = Flask(__name__)
CORS(app)  # Enable CORS for frontend

# Ngrok Setup
NGROK_AUTH_TOKEN = "2ubULXS9naq6POXMMzgU1XbfOzg_3KQeVtUvCXStF44u14uTc"  # Replace with your ngrok token
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
public_url = ngrok.connect(5000).public_url
print(f"Public ngrok URL: {public_url}")

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load Models
print("Loading Whisper model...")
model = whisper.load_model("base", device=device)
model.to(dtype=torch.float32)  # Force FP32 if using CPU

print("Loading Sentiment Model...")
sentiment_pipeline = pipeline("sentiment-analysis")

# Load spaCy for Named Entity Recognition
nlp = spacy.load("en_core_web_sm")

# Helper Functions

def convert_video_to_audio(video_path, audio_path):
    """Extracts audio from a video file."""
    clip = VideoFileClip(video_path)
    clip.audio.write_audiofile(audio_path)

def transcribe_audio(file_path):
    """Transcribes audio using Whisper"""
    result = model.transcribe(file_path)
    return result["text"]

def analyze_sentiment(text):
    """Analyzes sentiment using Hugging Face model, truncating long inputs"""
    max_length = 512  # Model's limit
    truncated_text = text[:max_length]  # Take only the first 512 characters
    sentiment_result = sentiment_pipeline(truncated_text)
    return sentiment_result[0]

def generate_wordcloud(text):
    """Generates a word cloud from the text"""
    wc = WordCloud(width=800, height=400, background_color='white').generate(text)
    img_io = io.BytesIO()
    plt.figure(figsize=(8, 4))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

def generate_bubble_chart(text):
    """Generates a bubble chart for word frequency"""
    words = word_tokenize(text)
    freq = Counter(words)
    df = pd.DataFrame(freq.items(), columns=['word', 'frequency'])
    df['size'] = MinMaxScaler((100, 1000)).fit_transform(df[['frequency']])
    plt.figure(figsize=(10, 6))
    plt.scatter(df['word'], df['frequency'], s=df['size'], alpha=0.5)
    plt.xticks(rotation=90)
    plt.title("Entity Type Bubble Chart")
    plt.ylabel("Frequency")
    plt.grid()
    img_io = io.BytesIO()
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

def generate_sentiment_over_time():
    """Generates a sentiment-over-time line chart."""
    timestamps = np.linspace(1, 100, num=10)  # Mock timestamps
    sentiments = np.random.uniform(-1, 1, size=10)  # Mock sentiment scores
    plt.figure(figsize=(10, 5))
    plt.plot(timestamps, sentiments, marker='o', linestyle='-')
    plt.xlabel("Time")
    plt.ylabel("Sentiment Score")
    plt.title("Sentiment Over Time")
    plt.grid(True)
    img_io = io.BytesIO()
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

# --- New Visualization Functions ---

def generate_named_entity_wordcloud(text):
    """Generates a word cloud from the named entities in the text"""
    doc = nlp(text)
    entities = " ".join([ent.text for ent in doc.ents])
    if not entities:
        entities = "None"
    wc = WordCloud(width=800, height=400, background_color='white').generate(entities)
    img_io = io.BytesIO()
    plt.figure(figsize=(8, 4))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis("off")
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

def generate_named_entity_frequency(text):
    """Generates a bar chart for the frequency of named entities"""
    doc = nlp(text)
    entities = [ent.text for ent in doc.ents]
    if not entities:
        entities = ["None"]
    freq = Counter(entities)
    labels, values = zip(*freq.most_common())
    plt.figure(figsize=(10, 6))
    plt.bar(labels, values, color='#0f0')
    plt.xticks(rotation=45, ha="right")
    plt.title("Named Entity Frequency")
    plt.ylabel("Frequency")
    plt.tight_layout()
    img_io = io.BytesIO()
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

def generate_emotion_heatmap(text):
    """Generates an emotion heatmap. For demo, we simulate an emotion matrix."""
    emotions = ['Happy', 'Sad', 'Angry', 'Neutral']
    data = np.random.rand(len(emotions), len(emotions))
    plt.figure(figsize=(8, 6))
    sns.heatmap(data, annot=True, xticklabels=emotions, yticklabels=emotions, cmap="YlGnBu")
    plt.title("Sentiment Emotion Heatmap")
    img_io = io.BytesIO()
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

def generate_ner_sentiment_sankey(text):
    """Generates a basic NER-sentiment Sankey diagram."""
    doc = nlp(text)
    sankey_data = {}
    for ent in doc.ents:
        result = sentiment_pipeline(ent.text)
        label = result[0]['label']
        sankey_data[ent.text] = label

    label_counts = Counter(sankey_data.values())
    from matplotlib.sankey import Sankey
    sankey = Sankey(unit=None)
    total = sum(label_counts.values())
    flows = [-total] + [count for count in label_counts.values()]
    labels = ['Entities'] + list(label_counts.keys())
    orientations = [0] + [1] * len(label_counts)
    sankey.add(flows=flows, labels=labels, orientations=orientations)
    plt.title("NER Sentiment Sankey Diagram")
    sankey.finish()
    img_io = io.BytesIO()
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

def generate_emotion_over_time():
    """Generates an emotion over time line chart."""
    timestamps = np.linspace(1, 20, num=20)
    emotions = np.sin(timestamps) + np.random.normal(0, 0.3, 20)
    plt.figure(figsize=(10, 5))
    plt.plot(timestamps, emotions, marker='o', linestyle='-')
    plt.xlabel("Time")
    plt.ylabel("Emotion Score")
    plt.title("Emotion Over Time")
    plt.grid(True)
    img_io = io.BytesIO()
    plt.savefig(img_io, format='png', bbox_inches='tight')
    plt.close()
    img_io.seek(0)
    return base64.b64encode(img_io.read()).decode('utf-8')

def generate_pdf(transcription, sentiment, wordcloud_img, bubble_chart, sentiment_time,
                 named_entity_wc, named_entity_freq, emotion_heatmap, ner_sankey, emotion_over_time):
    """Generates a PDF report with all charts"""
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, "Analysis Report", ln=True, align='C')
    pdf.ln(10)

    pdf.multi_cell(0, 10, f"Transcription:\n{transcription}")
    pdf.ln(5)
    pdf.multi_cell(0, 10, f"Sentiment Analysis:\n{sentiment}")
    pdf.ln(10)

    def add_image_to_pdf(img_data, title):
        temp_img = f"{title.replace(' ', '_').lower()}.png"
        with open(temp_img, "wb") as img_file:
            img_file.write(base64.b64decode(img_data))
        pdf.cell(200, 10, title, ln=True, align='C')
        pdf.image(temp_img, x=10, y=pdf.get_y(), w=150)
        pdf.ln(65)

    add_image_to_pdf(wordcloud_img, "General Word Cloud")
    add_image_to_pdf(bubble_chart, "Bubble Chart")
    add_image_to_pdf(sentiment_time, "Sentiment Over Time")
    add_image_to_pdf(named_entity_wc, "Named Entity Word Cloud")
    add_image_to_pdf(named_entity_freq, "Named Entity Frequency")
    add_image_to_pdf(emotion_heatmap, "Emotion Heatmap")
    add_image_to_pdf(ner_sankey, "NER Sentiment Sankey Diagram")
    add_image_to_pdf(emotion_over_time, "Emotion Over Time")

    pdf_path = "analysis_report.pdf"
    pdf.output(pdf_path)
    return pdf_path

# Routes

@app.route('/')
def home():
    return "Backend is running"

@app.route('/upload', methods=['POST'])
def upload_file():
    """Handles file upload and processing"""
    if 'file' not in request.files:
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files['file']
    file_path = os.path.join("uploads", file.filename)
    os.makedirs("uploads", exist_ok=True)
    file.save(file_path)

    # Convert video to audio if applicable
    if file.filename.lower().endswith(('mp4', 'avi', 'mov', 'mkv')):
        audio_path = file_path.rsplit('.', 1)[0] + ".wav"
        convert_video_to_audio(file_path, audio_path)
        file_path = audio_path

    transcription = transcribe_audio(file_path)
    sentiment = analyze_sentiment(transcription)
    # Generate visualizations
    wordcloud_img = generate_wordcloud(transcription)
    bubble_chart = generate_bubble_chart(transcription)
    sentiment_time = generate_sentiment_over_time()
    named_entity_wc = generate_named_entity_wordcloud(transcription)
    named_entity_freq = generate_named_entity_frequency(transcription)
    emotion_heatmap = generate_emotion_heatmap(transcription)
    ner_sankey = generate_ner_sentiment_sankey(transcription)
    emotion_over_time = generate_emotion_over_time()

    pdf_path = generate_pdf(transcription, sentiment, wordcloud_img, bubble_chart, sentiment_time,
                            named_entity_wc, named_entity_freq, emotion_heatmap, ner_sankey, emotion_over_time)

    return jsonify({
        "transcription": transcription,
        "sentiment": sentiment,
        "wordcloud": wordcloud_img,
        "bubble_chart": bubble_chart,
        "sentiment_time": sentiment_time,
        "named_entity_wordcloud": named_entity_wc,
        "named_entity_frequency": named_entity_freq,
        "emotion_heatmap": emotion_heatmap,
        "ner_sankey": ner_sankey,
        "emotion_over_time": emotion_over_time,
        "pdf_url": f"{public_url}/download"
    })

@app.route('/download', methods=['GET'])
def download_pdf():
    return send_file("analysis_report.pdf", as_attachment=True)

if __name__ == '__main__':
    app.run(port=5000)